# -*- coding: utf-8 -*-
"""Gallup_NLP_github

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1__z9Z5tkmINANQNTixUdePphaM9vmu38

[NLP project for Gallup] - built by JWS

Data
- Question-answer data from surveys
- e.g. ('자연어처리에 대하여 어떻게 생각하십니까?','재밌는 일이다')
- As the shape of input data is likely to be excel sheet and for usability of non-pythonic people, 
  this code is designed to encompass from preprocessing of raw excel data to getting a result by a few of clicks

Objective
- Clustering sentences(answer) having similar meanings

Method
- Embedding -> Fine Tuning -> Clustering

Method Details
- Embedding
 : Transformating setences to vectors by KoBERT
 : Use both question and answer
- Fine Tuning
 : Target values are not fixed but vary
 : Due to unfixed target values, it's hard to directly implement supervised learning
 : Therefore, some tricks as below are used for supervised learning
   > Firstly, clustering Xs (assigning an identical group number to Xs having similar meanings, let Zs the group)
   > e.g. (x1, x2, x3) = ('맛있다', '맛있네요', '맛있습니다') then  (z1, z2, z3) = (1, 1, 1)
   > Randomly select Xs having similar meanings(=identical group number) and get the average value of them(embedded vectors of Xs)
   > Calculating the average of (y1, y2, y3)
   > Use this average as a label for supervised learning
- Clustering
 : Using K-Means for clustering embedded vectors(Xs)
 : Choosing the number of cluster by representing the largest average silhouette value
"""

# Installing kobert

!pip3 install kobert-transformers
!pip3 install transformers

# Importing packages

from google.colab import files

import numpy as np
import pandas as pd
import re

from statistics import mean, median
from tracemalloc import stop

import torch
from torch.utils.data import DataLoader, TensorDataset

from statistics import mean, median
from tracemalloc import stop

from transformers import BertModel, BertTokenizer
from kobert_transformers import tokenization_kobert

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from torch.utils.data import DataLoader
from torch import nn
from torch.nn import functional

# Checking the availability of CUDA

print(torch.cuda.is_available())
print(torch.cuda.get_device_name())

# Uploading Datasets

upload = files.upload()

# Loading Datasets

df = pd.read_csv(r'')

# Class for setting input data(ids, attention masks)


class Gallup_NLP:
    # made by Wusuhk_Ju @ Gallup, 2022
    
    def __init__(self, df, question_col_name = '질문', answer_col_name = '응답', group_col_name = None, keys = 'keys', tokenizer_selection = 'kobert'):
        self.df = df.sort_values(by=keys)
        self.question_col_name = question_col_name
        self.raw_question = df[question_col_name].values
        self.answer_col_name = answer_col_name        
        self.raw_answer = df[answer_col_name].values        
        self.group_col_name = group_col_name
        self.keys = keys
        self.tokenizer_selection = tokenizer_selection        
        self.raw_keys = df[keys].values
        
        self.sentence_group = self._get_group()        
        self.tokenizer = self._get_tokenizer()
        
        self.cleaned_answer = None
        self.cleaned_question = None
        self.tokenized_answer = None
        self.token_length = None
        self.short_tokenized_answer = None
        self.short_tokenized_question = None
        self.normal_keys = None
        self.short_group = None
        self.input_for_tokens_answer = None
        self.attention_masks_answer = None
        self.input_for_tokens_question = None
        self.attention_masks_question = None

        self.ids_answer = None
        self.ids_question = None
        
    def _get_group(self):
        df = self.df
        group_col_name = self.group_col_name
        try:
            if group_col_name is not None:
                return df[group_col_name].values
            elif group_col_name is None:
                return None
        except:
            print('????????????? Please make sure the column name! ?????????????')
            return None            
        
    def _get_tokenizer(self):
        if self.tokenizer_selection == 'kobert':
            tz = tokenization_kobert.KoBertTokenizer.from_pretrained('monologg/kobert')
        elif self.tokenizer_selection == 'bert':
            tz = BertTokenizer.from_pretrained('bert-base-uncased')
        else:
            raise ValueError('????? Please select a tokenizer between "kobert" and "bert" for tokenizer_selection ?????')
        print('########## Tokenizer({}) has been set up ##########'.format(self.tokenizer_selection))
        return tz
                            
    def get_cleaned(self):       
        def implementation(input_sentences):
            array_raw_sentences = input_sentences
            array_cleaned_sentences = np.array([])
            s = 0
            for sentence in array_raw_sentences:
                s += 1
                array_cleaned_sentences = np.append(array_cleaned_sentences, re.sub("[^A-Za-z0-9가-힣 ]",'',sentence))
                print('Cleansing {}/{} has been done'.format(s, len(array_raw_sentences)))        
            return array_cleaned_sentences
            
        self.cleaned_answer = implementation(input_sentences=self.raw_answer)
        self.cleaned_question = implementation(input_sentences=self.raw_question)
        print('########## Sentences has been cleansed ##########')
    
    def get_tokenized(self):        
        tz = self.tokenizer 
        def implementaton(input_sentence):
            array_sentences = input_sentence
            lists_tokenized = []
            t = 0
            n_of_t = len(array_sentences)
            for sentence in array_sentences:
                t += 1
                tokenized = tz.tokenize(sentence)
                lists_tokenized.append(tokenized)
                print('Tokenization_answer for {}/{} has been done'.format(t, n_of_t))        
            return lists_tokenized
                               
        self.tokenized_answer = implementaton(self.cleaned_answer)
        self.tokenized_question = implementaton(self.cleaned_question)
        print('########## Tokenization has been done ##########')
                           
    def get_short_tokenized(self, cut_criteron = 'iqr'):                  
        raw_keys = self.raw_keys        
        tz = self.tokenizer   

        # ANSWER
        # cut_criteron = 'iqr' OR dictionary of two numbers, upper and lower ; {'upper':n, 'lower':n}
        lists_tokenized = self.tokenized_answer  
        list_length = [len(inner_list) for inner_list in lists_tokenized]                        
        
        if cut_criteron == 'iqr':
            q1 = np.quantile(list_length,0.25)
            q3 = np.quantile(list_length,0.75)
            iqr = q3 - q1        
            upper = q3 + 1.5*iqr
            lower = q1 - 1.5*iqr
            if lower < 0:
                lower = 0
                                                
            normal_idx = np.where((np.array(list_length) < upper) & (np.array(list_length) > lower))[0]
            normal_keys = raw_keys[normal_idx]
            n_normal = [1 if i >= 0 else 0 for i in normal_idx]
            n_outliers = len(lists_tokenized) - sum(n_normal)                                           
            
            lists_short_tokenized = []
            for idx in normal_idx:
                lists_short_tokenized.append(lists_tokenized[idx])            

        else:
            uplow_dic = cut_criteron
                        
            normal_idx = np.where((np.array(list_length) < uplow_dic['upper']) & (np.array(list_length) > uplow_dic['lower']))[0]
            normal_keys = raw_keys[normal_idx]            
            n_normal = [1 if i >= 0 else 0 for i in normal_idx]     
            n_outliers = len(lists_tokenized) - sum(normal_idx)
            
            lists_short_tokenized = []
            for idx in normal_idx:
                lists_short_tokenized.append(lists_tokenized[idx])
                                                   
        self.token_length = {'min': np.min(list_length), 'med': np.median(list_length), 'mean': np.mean(list_length), 'max': np.max(list_length), 
                             'n_of_outliers': n_outliers}
        self.short_tokenized_answer = lists_short_tokenized
        self.normal_keys = normal_keys        
        
        # QUESTION      
        cleaned_question = self.cleaned_question 
        list_questions = []
        for rk, q in zip(raw_keys, cleaned_question):
            if rk in normal_keys:                        
                list_questions.append(q)           
                
        lists_tokenized = []
        t = 0
        n_of_t = len(list_questions)
        for sentence in list_questions:
            t += 1
            tokenized = tz.tokenize(sentence)
            lists_tokenized.append(tokenized)
            print('Tokenization_question for {}/{} has been done'.format(t, n_of_t))
                
        self.short_tokenized_question = lists_tokenized                            
                                           
    def get_input_for_tokens(self):            
        def implementation(lists_input_short_tokens, cut_criterion):  
            # [PAD]                    
            lists_tokenized = lists_input_short_tokens
            temp_for_len = [len(inner_list) for inner_list in lists_tokenized]
            
            if cut_criterion == 'max':
                max_len = max(temp_for_len)
                
                lists_pad_tokenized = []
                p = 0
                n_of_p = len(lists_tokenized)
                for inner_list in lists_tokenized:
                    inner_list_temp = inner_list
                    p += 1                                    
                    if len(inner_list_temp) < max_len:
                        while len(inner_list_temp) < max_len:
                            inner_list_temp.append('[PAD]')
                    else:
                        pass
                    lists_pad_tokenized.append(inner_list_temp)
                    print('Padding for {}/{} has been done'.format(p, n_of_p))
                        
            elif cut_criterion == 'med':                
                med_len = int(median(temp_for_len))
                
                lists_pad_tokenized = []
                p = 0
                n_of_p = len(lists_tokenized)
                for inner_list in lists_tokenized:
                    inner_list_cut = inner_list[:med_len]
                    p += 1
                    if len(inner_list_cut) < med_len:
                        while len(inner_list_cut) < med_len:
                            inner_list_cut.append('[PAD]')
                    else:
                        pass
                    lists_pad_tokenized.append(inner_list_cut)
                    print('Padding for {}/{} has been done'.format(p, n_of_p))
                                
            # [CLS]
            lists_clspad_tokenized = []
            cls = 0
            n_of_cls = len(lists_pad_tokenized)                    
            for inner_list in lists_pad_tokenized:
                cls += 1
                inner_list.insert(0, '[CLS]')
                lists_clspad_tokenized.append(inner_list)
                print('Adding [CLS] for {}/{} has been done'.format(cls, n_of_cls))                        
            
            # [SEP]
            lists_clspadsep_tokenized = []
            sep = 0
            n_of_sep = len(lists_clspad_tokenized)          
            for inner_list in lists_clspad_tokenized:
                sep += 1
                inner_list.append('[SEP]')
                lists_clspadsep_tokenized.append(inner_list)
                print('Adding [SEP] for {}/{} has been done'.format(sep, n_of_sep))    

            # Attention mask
            lists_attention_masks = []
            a = 0
            n_of_a = len(lists_clspadsep_tokenized)
            for inner_list in lists_clspadsep_tokenized:
                a += 1
                attention_mask = [0 if token == '[PAD]' else 1 for token in inner_list]
                lists_attention_masks.append(attention_mask)
                print('Creating an attention mask for {}/{} has been done'.format(a, n_of_a))
                            
            return [lists_clspadsep_tokenized, lists_attention_masks]
            
        result1 = implementation(self.short_tokenized_answer, cut_criterion='max')
        self.input_for_tokens_answer = result1[0]
        self.attention_masks_answer = result1[1]

        result2 = implementation(self.short_tokenized_question, cut_criterion='med')
        self.input_for_tokens_question = result2[0]
        self.attention_masks_question = result2[1]
            
        print('########## Input tokens have been set up. Please check by "instance.input_for_tokens" ##########')
        print('########## Input attention masks have been set up. Please check by "instance.attention_mask" ##########')        
                            
    def change_tokens_to_ids(self):
        tz = self.tokenizer        
        def implementation(input_for_tokens):        
            lists_clspadsep_tokenized = input_for_tokens
        
            lists_ids = []
            for inner_list in lists_clspadsep_tokenized:
                list_ids = tz.convert_tokens_to_ids(inner_list)
                lists_ids.append(list_ids)
        
            return lists_ids

        self.ids_answer = implementation(self.input_for_tokens_answer)
        self.ids_question = implementation(self.input_for_tokens_question)
        
        print('########## Input ids_question have been set up. Please check by "instance.ids_answer or question" ##########')

    def get_short_group(self):
        normal_keys = self.normal_keys        
        raw_keys = self.raw_keys
        sentence_group = self.sentence_group        
        
        list_short_tokenized_group = []
        if sentence_group is not None:                
            for rk, sg in zip(raw_keys, sentence_group):
                if rk in normal_keys:                        
                    list_short_tokenized_group.append(sg)     
        elif sentence_group is None:
            list_short_tokenized_group = []            
        self.short_group = list_short_tokenized_group

# Function for checking errors

def error_test(NLP_inst):
    def implemantation(input_tokens):
        lists_tokenized = input_tokens
        len_tk = len(lists_tokenized)

        n = 0
        for _ in range(10):
            n += 1
            random_int_1 = np.random.randint(len_tk)
            random_int_2 = np.random.randint(len_tk)
            len_1 = len(lists_tokenized[random_int_1])
            len_2 = len(lists_tokenized[random_int_2])
            if len_1 == len_2:
                print('test...{}/{}'.format(n,10))
            else:
                stop
                print('????????? {} and {} unmatched! ?????????'.format(random_int_1, random_int_2))
    
    implemantation(NLP_inst.input_for_tokens_answer)
    implemantation(NLP_inst.input_for_tokens_question)
    print('########## No errors have been found :) ##########')

# !!!!!!!!!!!!!!!!!! GPU MUST !!!!!!!!!!!!!!!!!!!!!!!!
# Embedding ids and attention masks by KoBERT
# This should be implemented in several times


class Embedding:
    def __init__(self, gallup_nlp_instance, device_setting = 'cuda', model_selection = 'kobert', n_of_hiddne_layers=4):
        self.nlp_instance = gallup_nlp_instance
        self.device_setting = device_setting
        self.model_selection = model_selection
        self.n_of_hiddne_layers = n_of_hiddne_layers
        
        self.device = self._set_device()
        self.model = None            
        
        self.cnn_dataset_total = None
        
        self.last_hidden_layers_answer = None
        self.cls_tanh_layers_answer = None
        self.all_hidden_layers_answer = None
        self.cnn_dataset_answer = None
        
        self.last_hidden_layers_question = None
        self.cls_tanh_layers_question = None
        self.all_hidden_layers_question = None
        self.cnn_dataset_question = None

        self.y = None
        
    def _set_device(self):
        if self.device_setting == 'cuda':
            device = torch.device('cuda:0')
        else:
            device = torch.device('cpu')
        return device
        
    def get_model(self):        
        if self.model_selection == 'kobert':
            model = BertModel.from_pretrained('monologg/kobert')
        elif self.model_selection == 'bert':
            model = BertModel.from_pretrained('bert-base-uncased')
        self.model = model
        print('BertModel.from_pretrained({}) has been set up'.format(self.model_selection))
                    
    def get_embedded_tensors(self):
        def implementation(answer_or_question, model, device, n_of_hiddne_layers=self.n_of_hiddne_layers):
            model = self.model
            device = self.device
            last_len = (12-n_of_hiddne_layers+1)
        
            if answer_or_question == 'answer':
                lists_ids = self.nlp_instance.ids_answer
                lists_attention_masks = self.nlp_instance.attention_masks_answer
            elif answer_or_question == 'question':
                lists_ids = self.nlp_instance.ids_question
                lists_attention_masks = self.nlp_instance.attention_masks_question
                        
            tensor_ids = torch.Tensor(lists_ids).type(torch.int32)
            tensor_masks = torch.Tensor(lists_attention_masks).type(torch.int32)
        
            model_device = model.to(device)
            tensor_last_hidden_layers = torch.Tensor()
            tensor_cls_tanh_layers = torch.Tensor()
            list_all_hidden_layers = []
        
            for i in range(len(tensor_ids)):            
                len_sequence = tensor_ids[i].shape[0]
                id = tensor_ids[i].reshape(1, len_sequence).to(device)
                mask = tensor_masks[i].reshape(1, len_sequence).to(device)
                                                                        
                with torch.no_grad():
                    output = model_device(input_ids = id, attention_mask = mask, output_hidden_states = True)
                last = output[0].to('cpu') 
                cls = output[1].to('cpu')
            
                tensor_last_hidden_layers = torch.concat([ tensor_last_hidden_layers, last ])
                tensor_cls_tanh_layers = torch.concat([ tensor_cls_tanh_layers, cls ])            
                list_all_hidden_layers.append(output[2][last_len:])
                                   
                print('Embedding {}/{} has been done'.format(i+1, len(tensor_ids)))
                
            return [tensor_last_hidden_layers, tensor_cls_tanh_layers, list_all_hidden_layers]

        result1 = implementation('answer', self.model, self.device)
        self.last_hidden_layers_answer = result1[0] # <<<<<<<
        cls_tanh_layers_answer = result1[1]
        sample_size = cls_tanh_layers_answer.shape[0]
        feature_size = cls_tanh_layers_answer.shape[1]
        self.cls_tanh_layers_answer = cls_tanh_layers_answer.reshape(shape = (sample_size,1,feature_size)) # <<<<<<<
        self.all_hidden_layers_answer = result1[2] # <<<<<<<

        result2 = implementation('question',self.model, self.device)
        self.last_hidden_layers_question = result2[0] # <<<<<<<
        cls_tanh_layers_question = result2[1]
        sample_size = cls_tanh_layers_question.shape[0]
        feature_size = cls_tanh_layers_question.shape[1]
        self.cls_tanh_layers_question = cls_tanh_layers_question.reshape(shape = (sample_size,1,feature_size)) # <<<<<<<
        self.all_hidden_layers_question = result2[2] # <<<<<<<
           
    def get_cnn_dataset(self, n_of_hidden_layers = 4, erase_all_hidden_layers = False):

        all_hidden_layers_a = self.all_hidden_layers_answer                
        token_len_a = len(self.nlp_instance.ids_answer[0])

        all_hidden_layers_q = self.all_hidden_layers_question
        token_len_q = len(self.nlp_instance.ids_question[0])
                
        tensor_cnn_dataset = torch.Tensor()
        tensor_cnn_dataset_q = torch.Tensor()
        tensor_cnn_dataset_a = torch.Tensor()
        for q, a in zip(all_hidden_layers_q, all_hidden_layers_a):
            inner_ten = torch.Tensor()
            inner_ten_q = torch.Tensor()
            inner_ten_a = torch.Tensor()
            for j in range(n_of_hidden_layers):
                try:
                    inner_ten = torch.concat([ inner_ten, q[j].to('cpu') ])
                    inner_ten = torch.concat([ inner_ten, a[j].to('cpu') ], dim=1 )
                    inner_ten_q = torch.concat([ inner_ten_q, q[j].to('cpu') ])
                    inner_ten_a = torch.concat([ inner_ten_a, a[j].to('cpu') ])
                    
                except: 
                    inner_ten = torch.concat([ inner_ten, q[j].to('cpu') ], dim=1)
                    inner_ten = torch.concat([ inner_ten, a[j].to('cpu') ], dim=1 )   
                    inner_ten_q = torch.concat([ inner_ten_q, q[j].to('cpu') ])
                    inner_ten_a = torch.concat([ inner_ten_a, a[j].to('cpu') ])                                     
            
            inner_ten = inner_ten.reshape( (1, n_of_hidden_layers, (token_len_q + token_len_a), 768) )   
            inner_ten_q = inner_ten_q.reshape( (1,n_of_hidden_layers,token_len_q,768) )     
            inner_ten_a = inner_ten_a.reshape( (1,n_of_hidden_layers,token_len_a,768) )     
            
            tensor_cnn_dataset = torch.concat( [ tensor_cnn_dataset, inner_ten ] ,dim=0 )
            tensor_cnn_dataset_q = torch.concat( [ tensor_cnn_dataset_q, inner_ten_q ] ,dim=0 )
            tensor_cnn_dataset_a = torch.concat( [ tensor_cnn_dataset_a, inner_ten_a ] ,dim=0 )
            
        if erase_all_hidden_layers == True:
            self.all_hidden_layers = '### Cleaned for memory. Please check out the value "inst.cnn_dataset" ###'
            self.all_hidden_layers_answer = '### Cleaned for memory. Please check out the value "inst.cnn_dataset" ###'
            self.all_hidden_layers_question = '### Cleaned for memory. Please check out the value "inst.cnn_dataset" ###'
        
        self.cnn_dataset_total = tensor_cnn_dataset
        self.cnn_dataset_answer = tensor_cnn_dataset_a
        self.cnn_dataset_question = tensor_cnn_dataset_q
        
    def get_y(self, num_of_x_for_mean = 10):
        nlp_instance = self.nlp_instance
        tensor_x = self.cls_tanh_layers_answer
           
        tensor_y = torch.Tensor()
        s = 0
        for group_num in nlp_instance.short_group:
            s += 1
            array_idxs = np.where( np.array(nlp_instance.short_group) == group_num )[0] # Finding indices having the same group as x
    
            list_group_num_idxs = []
            for _ in range(num_of_x_for_mean):
                list_group_num_idxs.append( array_idxs[np.random.randint( 0, len(array_idxs) )] ) # Randomly select n of indices 

            tensor_mean = torch.zeros(size = (1,1,1,768))
            for idx in list_group_num_idxs:
                tensor_mean += tensor_x[idx]
        
            tensor_mean = tensor_mean / num_of_x_for_mean
            tensor_mean = tensor_mean.reshape(shape = (1,1,1,768))
    
            try:
                tensor_y = torch.concat( [tensor_y, tensor_mean] )
            except:
                tensor_y = tensor_mean
            
            print('Y for {}/{} has been created'.format(s, len(tensor_x)))
        
        self.y = tensor_y
        print('########## Y has been set up ##########')

# Learned model will be here

# Gallup_NLP instance

nlp_inst = Gallup_NLP(df = df, question_col_name='질문', answer_col_name='응답', group_col_name = None, keys='key', tokenizer_selection='kobert') # group_col_name is used only in learning
nlp_inst.get_cleaned_sentences()
nlp_inst.get_tokenized()
nlp_inst.get_short_tokenized()
nlp_inst.get_input_for_tokens()
nlp_inst.change_tokens_to_ids()

error_test(answer_inst)

# Getting final dataset for learning

embedding_inst = Embedding(nlp_inst, device_setting='cuda', model_selection='kobert', n_of_hidden_layers=4)
embedding_inst.get_model()
embedding_inst.get_embedded_tensors()
embedding_inst.get_cnn_dataset()
embedding_inst.get_y(num_of_x_mean = 10)