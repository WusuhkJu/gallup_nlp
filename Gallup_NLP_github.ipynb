{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gallup_NLP_github",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[NLP project for Gallup] - built by JWS\n",
        "\n",
        "Objective\n",
        "- Clustering sentences having similar meanings\n",
        "\n",
        "Method\n",
        "- Embedding -> Fine Tuning -> Clustering\n",
        "\n",
        "Method Details\n",
        "- Embedding\n",
        " : Transformating setences to vectors by KoBERT\n",
        "- Fine Tuning\n",
        " : Target values are not fixed but vary\n",
        " : Due to unfixed target values, it's hard to directly implement supervised learning\n",
        " : Therefore, some tricks as below are used for supervised learning\n",
        "   > Firstly, clustering Xs (assigning an identical group number to Xs having similar meanings, let Z the group number vector)\n",
        "   > e.g. (x1, x2, x3) = ('맛있다', '맛있네요', '맛있습니다') then  (z1, z2, z3) = (1, 1, 1)\n",
        "   > Randomly select Xs having similar meanings(=identical group number) and get the average value of them(embedded vectors of Xs)\n",
        "   > Using the average vector above as (y1, y2, y3)\n",
        "- Clustering\n",
        " : Using K-Means for clustering embedded vectors(Xs)\n",
        " : Choosing the number of cluster by representing the largest average silhouette value\n",
        "  \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mhpQ-zuE8d0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing kobert\n",
        "\n",
        "!pip3 install kobert-transformers\n",
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "FkEFbuv7DiG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing packages\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from statistics import mean, median\n",
        "from tracemalloc import stop\n",
        "\n",
        "import torch\n",
        "\n",
        "from statistics import mean, median\n",
        "from tracemalloc import stop\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from kobert_transformers import tokenization_kobert\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import functional"
      ],
      "metadata": {
        "id": "AVOFL7TgDjxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the availability of CUDA\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name())"
      ],
      "metadata": {
        "id": "fvTfQwWfDoVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading Datasets\n",
        "\"\"\"\n",
        "upload = files.upload()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4MypuiWYDoSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Datasets\n",
        "\"\"\"\n",
        "ids = np.load(r'')\n",
        "attention_masks = np.load(r'')\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv(r'')"
      ],
      "metadata": {
        "id": "J1LIf5BCDoO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for setting input data(ids, attention masks)\n",
        "\n",
        "class Gallup_NLP:\n",
        "    # made by Wusuhk_Ju @ Gallup, 2022\n",
        "    \n",
        "    def __init__(self, df, answer_col_name = '응답', group_col_name = None, tokenizer_selection = 'kobert'):\n",
        "        self.df = df\n",
        "        self.answer_col_name = answer_col_name\n",
        "        self.raw_sentences = df[answer_col_name].values\n",
        "        self.group_col_name = group_col_name\n",
        "        self.tokenizer_selection = tokenizer_selection        \n",
        "        \n",
        "        self.sentence_group = self._get_group()\n",
        "        self.cleaned_sentences = self._get_cleaned_sentences()                \n",
        "        self.tokenizer = self._get_tokenizer()\n",
        "        \n",
        "        self.tokenized = None\n",
        "        self.token_length = None\n",
        "        self.short_tokenized = None\n",
        "        self.short_group = None\n",
        "        self.input_for_tokens = None\n",
        "        self.attention_masks = None\n",
        "        self.ids = None\n",
        "        \n",
        "    def _get_group(self):\n",
        "        df = self.df\n",
        "        group_col_name = self.group_col_name\n",
        "        try:\n",
        "            if group_col_name is not None:\n",
        "                return df[group_col_name].values\n",
        "            elif group_col_name is None:\n",
        "                return None\n",
        "        except:\n",
        "            print('????????????? Please make sure the column name! ?????????????')\n",
        "            return None            \n",
        "                    \n",
        "    def _get_cleaned_sentences(self):\n",
        "        array_raw_sentences = self.raw_sentences\n",
        "        array_cleaned_sentences = np.array([])\n",
        "        s = 0\n",
        "        for sentence in array_raw_sentences:\n",
        "            s += 1\n",
        "            array_cleaned_sentences = np.append(array_cleaned_sentences, re.sub(\"[^A-Za-z0-9가-힣 ]\",'',sentence))\n",
        "            print('Cleansing {}/{} has been done'.format(s, len(array_raw_sentences)))\n",
        "        print('########## Sentences for analysis has been set up ##########')\n",
        "        return array_cleaned_sentences \n",
        "        \n",
        "    def _get_tokenizer(self):\n",
        "        if self.tokenizer_selection == 'kobert':\n",
        "            tz = tokenization_kobert.KoBertTokenizer.from_pretrained('monologg/kobert')\n",
        "        elif self.tokenizer_selection == 'bert':\n",
        "            tz = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        else:\n",
        "            raise ValueError('????? Please select a tokenizer between \"kobert\" and \"bert\" for tokenizer_selection ?????')\n",
        "        print('########## Tokenizer({}) has been set up ##########'.format(self.tokenizer_selection))\n",
        "        return tz\n",
        "    \n",
        "    def get_tokenized(self, cleaned_or_raw_sentences = 'cleaned'):\n",
        "        tz = self.tokenizer        \n",
        "        if cleaned_or_raw_sentences == 'cleaned':\n",
        "            array_sentences = self.cleaned_sentences\n",
        "        elif cleaned_or_raw_sentences == 'raw':\n",
        "            array_sentences = self.raw_sentences\n",
        "        else:\n",
        "            raise ValueError('????? Please select target setences to be tokenized between \"cleaned\" and \"raw\" ?????')\n",
        "        \n",
        "        # 1. Creating tokens\n",
        "        lists_tokenized = []\n",
        "        t = 0\n",
        "        n_of_t = len(array_sentences)\n",
        "        for sentence in array_sentences:\n",
        "            t += 1\n",
        "            tokenized = tz.tokenize(sentence)\n",
        "            lists_tokenized.append(tokenized)\n",
        "            print('Tokenization for {}/{} has been done'.format(t, n_of_t))\n",
        "        \n",
        "        self.tokenized = lists_tokenized\n",
        "        \n",
        "    def get_short_tokenized(self, cut_criteron = 'iqr'):\n",
        "        # cut_criteron = 'iqr' OR dictionary of two numbers, upper and lower ; {'upper':n, 'lower':n}\n",
        "        lists_tokenized = self.tokenized\n",
        "        sentence_group = self.sentence_group\n",
        "        \n",
        "        list_length = [len(inner_list) for inner_list in lists_tokenized]                        \n",
        "        \n",
        "        if cut_criteron == 'iqr':\n",
        "            q1 = np.quantile(list_length,0.25)\n",
        "            q3 = np.quantile(list_length,0.75)\n",
        "            iqr = q3 - q1        \n",
        "            upper = q3 + 1.5*iqr\n",
        "            lower = q1 - 1.5*iqr\n",
        "            if lower < 0:\n",
        "                lower = 0\n",
        "            \n",
        "            normal_idx = np.where((np.array(list_length) < upper) & (np.array(list_length) > lower))[0]\n",
        "            n_normal = [1 if i >= 0 else 0 for i in normal_idx]\n",
        "            n_outliers = len(lists_tokenized) - sum(n_normal)        \n",
        "            \n",
        "            lists_short_tokenized = []\n",
        "            for idx in normal_idx:\n",
        "                lists_short_tokenized.append(lists_tokenized[idx])\n",
        "                \n",
        "            list_short_tokenized_group = []\n",
        "            for idx in normal_idx:\n",
        "                list_short_tokenized_group.append(sentence_group[idx])\n",
        "                \n",
        "        else:\n",
        "            uplow_dic = cut_criteron\n",
        "            \n",
        "            normal_idx = np.where((np.array(list_length) < uplow_dic['upper']) & (np.array(list_length) > uplow_dic['lower']))[0]\n",
        "            n_normal = [1 if i >= 0 else 0 for i in normal_idx]     \n",
        "            n_outliers = len(lists_tokenized) - sum(normal_idx)\n",
        "            \n",
        "            lists_short_tokenized = []\n",
        "            for idx in normal_idx:\n",
        "                lists_short_tokenized.append(lists_tokenized[idx])\n",
        "                \n",
        "            list_short_tokenized_group = []\n",
        "            for idx in normal_idx:\n",
        "                list_short_tokenized_group.append(sentence_group[idx])\n",
        "                        \n",
        "        self.token_length = {'min': np.min(list_length), 'med': np.median(list_length), 'mean': np.mean(list_length), 'max': np.max(list_length), \n",
        "                             'n_of_outliers': n_outliers}\n",
        "        self.short_tokenized = lists_short_tokenized\n",
        "        self.short_group = list_short_tokenized_group\n",
        "                                   \n",
        "    def get_input_for_tokens(self):     \n",
        "        lists_tokenized = self.short_tokenized\n",
        "             \n",
        "        # 2. [PAD]                    \n",
        "        temp_for_len = [len(inner_list) for inner_list in lists_tokenized]\n",
        "        max_len = max(temp_for_len)            \n",
        "        lists_pad_tokenized = []\n",
        "        p = 0\n",
        "        n_of_p = len(lists_tokenized)\n",
        "        for inner_list in lists_tokenized:\n",
        "            p += 1\n",
        "            if len(inner_list) < max_len:\n",
        "                while len(inner_list) < max_len:\n",
        "                    inner_list.append('[PAD]')\n",
        "            else:\n",
        "                pass\n",
        "            lists_pad_tokenized.append(inner_list)\n",
        "            print('Padding for {}/{} has been done'.format(p, n_of_p))\n",
        "\n",
        "        # 3. [CLS]\n",
        "        lists_clspad_tokenized = []\n",
        "        cls = 0\n",
        "        n_of_cls = len(lists_pad_tokenized)                    \n",
        "        for inner_list in lists_pad_tokenized:\n",
        "            cls += 1\n",
        "            inner_list.insert(0, '[CLS]')\n",
        "            lists_clspad_tokenized.append(inner_list)\n",
        "            print('Adding [CLS] for {}/{} has been done'.format(cls, n_of_cls))                        \n",
        "            \n",
        "        # 4. [SEP]\n",
        "        lists_clspadsep_tokenized = []\n",
        "        sep = 0\n",
        "        n_of_sep = len(lists_clspad_tokenized)          \n",
        "        for inner_list in lists_clspad_tokenized:\n",
        "            sep += 1\n",
        "            inner_list.append('[SEP]')\n",
        "            lists_clspadsep_tokenized.append(inner_list)\n",
        "            print('Adding [SEP] for {}/{} has been done'.format(sep, n_of_sep))    \n",
        "\n",
        "        # 5. Attention mask\n",
        "        lists_attention_masks = []\n",
        "        a = 0\n",
        "        n_of_a = len(lists_clspadsep_tokenized)\n",
        "        for inner_list in lists_clspadsep_tokenized:\n",
        "            a += 1\n",
        "            attention_mask = [0 if token == '[PAD]' else 1 for token in inner_list]\n",
        "            lists_attention_masks.append(attention_mask)\n",
        "            print('Creating an attention mask for {}/{} has been done'.format(a, n_of_a))\n",
        "                            \n",
        "        self.input_for_tokens = lists_clspadsep_tokenized\n",
        "        self.attention_masks = lists_attention_masks\n",
        "        \n",
        "        print('########## Input tokens have been set up. Please check by \"instance.input_for_tokens\" ##########')\n",
        "        print('########## Input attention masks have been set up. Please check by \"instance.attention_mask\" ##########')\n",
        "        \n",
        "    def change_tokens_to_ids(self):\n",
        "        tz = self.tokenizer\n",
        "        lists_clspadsep_tokenized = self.input_for_tokens\n",
        "        \n",
        "        lists_ids = []\n",
        "        for inner_list in lists_clspadsep_tokenized:\n",
        "            list_ids = tz.convert_tokens_to_ids(inner_list)\n",
        "            lists_ids.append(list_ids)\n",
        "        \n",
        "        self.ids = lists_ids"
      ],
      "metadata": {
        "id": "LdNL0MRnDoLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for checking errors\n",
        "\n",
        "def error_test(NLP_inst):\n",
        "    lists_tokenized = NLP_inst.input_for_tokens\n",
        "    len_tk = len(lists_tokenized)\n",
        "\n",
        "    # Do all tokens have the same length? (Does padding )\n",
        "    n = 0\n",
        "    for _ in range(10):\n",
        "        n += 1\n",
        "        random_int_1 = np.random.randint(len_tk)\n",
        "        random_int_2 = np.random.randint(len_tk)\n",
        "        len_1 = len(NLP_inst.input_for_tokens[random_int_1])\n",
        "        len_2 = len(NLP_inst.input_for_tokens[random_int_2])\n",
        "        if len_1 == len_2:\n",
        "            print('test...{}/{}'.format(n,10))\n",
        "        else:\n",
        "            stop\n",
        "            print('????????? {} and {} unmatched! ?????????'.format(random_int_1, random_int_2))\n",
        "    print('########## No errors have been found :) ##########')"
      ],
      "metadata": {
        "id": "hC5mXxQIDoH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gallup_NLP instance\n",
        "\n",
        "answer_inst = Gallup_NLP(df, answer_col_name = '응답', group_col_name = '응답그룹', tokenizer_selection = 'kobert')\n",
        "answer_inst.get_tokenized(cleaned_or_raw_sentences = 'cleaned')\n",
        "answer_inst.get_short_tokenized(cut_criteron = 'iqr')\n",
        "answer_inst.get_input_for_tokens()\n",
        "answer_inst.change_tokens_to_ids()\n",
        "\n",
        "error_test(answer_inst)"
      ],
      "metadata": {
        "id": "sMScxcNVDoD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !!!!!!!!!!!!!!!!!! GPU MUST !!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# Embedding ids and attention masks by KoBERT\n",
        "# This should be implemented in several times\n",
        "\n",
        "class Embedding:\n",
        "    def __init__(self, gallup_nlp_instance, device_setting = 'cuda', model_selection = 'kobert'):\n",
        "        self.nlp_instance = gallup_nlp_instance\n",
        "        self.device_setting = device_setting\n",
        "        self.model_selection = model_selection\n",
        "        \n",
        "        self.device = self._set_device()\n",
        "        self.model = None            \n",
        "        self.last_hidden_layers = None\n",
        "        self.cls_tanh_layers = None\n",
        "        self.all_hidden_layers = None\n",
        "        self.cnn_dataset = None\n",
        "        self.y = None\n",
        "        \n",
        "    def _set_device(self):\n",
        "        if self.device_setting == 'cuda':\n",
        "            device = torch.device('cuda:0')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        return device\n",
        "        \n",
        "    def get_model(self):        \n",
        "        if self.model_selection == 'kobert':\n",
        "            model = BertModel.from_pretrained('monologg/kobert')\n",
        "        elif self.model_selection == 'bert':\n",
        "            model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.model = model\n",
        "        print('BertModel.from_pretrained({}) has been set up'.format(self.model_selection))\n",
        "                    \n",
        "    def get_embedded_tensors(self):\n",
        "        nlp_instance = self.nlp_instance\n",
        "        model = self.model\n",
        "        device = self.device\n",
        "        \n",
        "        lists_ids = nlp_instance.ids\n",
        "        lists_attention_masks = nlp_instance.attention_masks\n",
        "        \n",
        "        tensor_ids = torch.Tensor(lists_ids).type(torch.int32)\n",
        "        tensor_masks = torch.Tensor(lists_attention_masks).type(torch.int32)\n",
        "        \n",
        "        model_device = model.to(device)\n",
        "        tensor_last_hidden_layers = torch.Tensor()\n",
        "        tensor_cls_tanh_layers = torch.Tensor()\n",
        "        list_all_hidden_layers = []\n",
        "        \n",
        "        for i in range(len(tensor_ids)):            \n",
        "            len_sequence = tensor_ids[i].shape[0]\n",
        "            id = tensor_ids[i].reshape(1, len_sequence).to(device)\n",
        "            mask = tensor_masks[i].reshape(1, len_sequence).to(device)\n",
        "                                                                        \n",
        "            with torch.no_grad():\n",
        "                output = model_device(input_ids = id, attention_mask = mask, output_hidden_states = True)\n",
        "            last = output[0].to('cpu')\n",
        "            cls = output[1].to('cpu')\n",
        "            \n",
        "            tensor_last_hidden_layers = torch.concat([ tensor_last_hidden_layers, last ])\n",
        "            tensor_cls_tanh_layers = torch.concat([ tensor_cls_tanh_layers, cls ])            \n",
        "            list_all_hidden_layers.append(output[2])\n",
        "                                   \n",
        "            print('Embedding {}/{} has been done'.format(i+1, len(tensor_ids)))\n",
        "        \n",
        "        self.last_hidden_layers = tensor_last_hidden_layers\n",
        "        self.cls_tanh_layers = tensor_cls_tanh_layers\n",
        "        \n",
        "        sample_size = tensor_cls_tanh_layers.shape[0]\n",
        "        token_length = 1\n",
        "        feature_size = tensor_cls_tanh_layers.shape[1]\n",
        "        self.cls_tanh_layers = tensor_cls_tanh_layers.reshape(shape = (sample_size, token_length, feature_size))\n",
        "        \n",
        "        self.all_hidden_layers = list_all_hidden_layers\n",
        "        self.model = model_device\n",
        "        \n",
        "    def get_cnn_dataset(self, n_of_hidden_layers = 4, cls_adding = True, stack_way = 1, erase_all_hidden_layers = False):\n",
        "        all_hidden_layers = self.all_hidden_layers\n",
        "        cls_tanh_layers = self.cls_tanh_layers\n",
        "        temp_range = 12 - n_of_hidden_layers\n",
        "        \n",
        "        tensor_cnn_dataset = torch.Tensor()\n",
        "        for i in range(len(all_hidden_layers)):\n",
        "            inner_ten = torch.Tensor()\n",
        "            if cls_adding:\n",
        "                inner_ten = torch.concat( [ inner_ten, cls_tanh_layers[i].reshape( shape = (1,1,768) ) ] )\n",
        "            for j in range(12,temp_range,-1):\n",
        "                inner_ten = torch.concat( [ inner_ten, all_hidden_layers[i][j].to('cpu') ], dim=stack_way )\n",
        "            tensor_cnn_dataset = torch.concat( [ tensor_cnn_dataset, inner_ten ] ,dim=0 )\n",
        "            \n",
        "        if erase_all_hidden_layers == True:\n",
        "            self.all_hidden_layers = '### Cleaned for memory. Please check out the value \"inst.cnn_dataset\" ###'\n",
        "        \n",
        "        self.cnn_dataset = tensor_cnn_dataset\n",
        "        \n",
        "    def get_y(self, type_of_x = 'cnn_dataset', num_of_x_for_mean = 10):\n",
        "        nlp_instance = self.nlp_instance\n",
        "        \n",
        "        if type_of_x == 'cnn_dataset':\n",
        "            tensor_x = self.cnn_dataset\n",
        "        elif type_of_x == 'cls_tanh_layers':\n",
        "            tensor_x = self.cls_tanh_layers\n",
        "        else:\n",
        "            print('Please choose the type of the dataset between cnn_dataset and cls_tanh_layers')\n",
        "            \n",
        "        tensor_y = torch.Tensor()\n",
        "        s = 0\n",
        "        for group_num in nlp_instance.short_group:\n",
        "            s += 1\n",
        "            array_idxs = np.where( np.array(nlp_instance.short_group) == group_num )[0] # Finding indices having the same group as x\n",
        "    \n",
        "            list_group_num_idxs = []\n",
        "            for _ in range(num_of_x_for_mean):\n",
        "                list_group_num_idxs.append( array_idxs[np.random.randint( 0, len(array_idxs) )] ) # Randomly select n of indices \n",
        "\n",
        "            tensor_size = tensor_x[0].shape\n",
        "            tensor_mean = torch.zeros(size = tensor_size)\n",
        "            for idx in list_group_num_idxs:\n",
        "                tensor_mean += tensor_x[idx]\n",
        "        \n",
        "            tensor_mean = tensor_mean / num_of_x_for_mean\n",
        "            tensor_mean = tensor_mean.reshape(shape = (1, tensor_size[0], tensor_size[1]))\n",
        "    \n",
        "            try:\n",
        "                tensor_y = torch.concat( [tensor_y, tensor_mean] )\n",
        "            except:\n",
        "                tensor_y = tensor_mean\n",
        "            \n",
        "            print('Y for {}/{} has been created'.format(s, len(tensor_x)))\n",
        "        \n",
        "        self.y = tensor_y\n",
        "        print('########## Y has been set up ##########')"
      ],
      "metadata": {
        "id": "mvChMtsiEodb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting final dataset for learning\n",
        "\n",
        "embedding_inst = Embedding(nlp_inst, device_setting = 'cuda')\n",
        "embedding_inst.get_model()\n",
        "embedding_inst.get_embedded_tensors()\n",
        "embedding_inst.get_cnn_dataset()\n",
        "embedding_inst.get_cnn_dataset(cls_adding = False)\n",
        "embedding_inst.get_y(type_of_x = 'cls_tanh_layers')"
      ],
      "metadata": {
        "id": "cUdKT1NKEoZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader will be used here"
      ],
      "metadata": {
        "id": "cFTXD3PuEoVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lYU2FpKrDoAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s-Vlyu1VDn7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oJFs2FA0Dn0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsIkJQMz8QFv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}